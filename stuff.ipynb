{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Once', 'upon', 'time', 'there', 'was', 'a', 'hippo', 'named', 'Alfred', '.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = nltk.tokenize.word_tokenize(\"Once upon time there was a hippo named Alfred.\")\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hey', 'PRP'),\n",
       " (',', ','),\n",
       " ('can', 'MD'),\n",
       " ('I', 'PRP'),\n",
       " ('have', 'VB'),\n",
       " ('that', 'IN'),\n",
       " ('can', 'MD'),\n",
       " ('of', 'IN'),\n",
       " ('olives', 'NNS'),\n",
       " ('?', '.')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hey', 'PRP'),\n",
       " (',', ','),\n",
       " ('can', 'MD'),\n",
       " ('I', 'PRP'),\n",
       " ('have', 'VB'),\n",
       " ('that', 'IN'),\n",
       " ('can', 'MD'),\n",
       " ('of', 'IN'),\n",
       " ('olives', 'NNS'),\n",
       " ('?', '.')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tagger gets confused\n",
    "text = nltk.tokenize.word_tokenize(\"Hey, can I have that can of olives?\")\n",
    "#text = nltk.tokenize.word_tokenize(\"Can I have that can of olives?\")\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can: 94\n",
      "could: 87\n",
      "may: 93\n",
      "might: 38\n",
      "must: 53\n",
      "will: 389\n",
      "butter: 2\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "news_text = brown.words(categories='news')\n",
    "fdit = nltk.FreqDist(w.lower() for w in news_text)\n",
    "modals = ['can', 'could', 'may', 'might', 'must', 'will', 'butter']\n",
    "for m in modals:\n",
    "    print m + ':', fdit[m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Once', 'upon'),\n",
       " ('upon', 'a'),\n",
       " ('a', 'time'),\n",
       " ('time', 'there'),\n",
       " ('there', 'was'),\n",
       " ('was', 'a'),\n",
       " ('a', 'hippo'),\n",
       " ('hippo', 'named'),\n",
       " ('named', 'alfed')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"Once upon a time there was a hippo named alfed\".split()\n",
    "list(nltk.bigrams(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## some stuff from MAS and the nltk intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'BROWN CORPUS\\n\\nA Standard Corpus of Present-Day Edited American\\nEnglish, for use with Digital Computers.\\n\\nby W. N. Francis and H. Kucera (1964)\\nDepartment of Linguistics, Brown University\\nProvidence, Rhode Island, USA\\n\\nRevised 1971, Revised and Amplified 1979\\n\\nhttp://www.hit.uib.no/icame/brown/bcm.html\\n\\nDistributed with the permission of the copyright holder,\\nredistribution permitted.\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.readme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260818"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(nltk.bigrams(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260819"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260817"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(nltk.trigrams(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'[', u'Moby', u'Dick', u'by', u'Herman', u'Melville', ...]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?nltk.ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'[', u'Moby', u'Dick', u'by', u'Herman', u'Melville', ...]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "gutenberg.fileids()\n",
    "words = gutenberg.words('melville-moby_dick.txt')\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sperm Whale; Moby Dick; White Whale; old man; Captain Ahab; sperm\n",
      "whale; Right Whale; Captain Peleg; New Bedford; Cape Horn; cried Ahab;\n",
      "years ago; lower jaw; never mind; Father Mapple; cried Stubb; chief\n",
      "mate; white whale; ivory leg; one hand\n"
     ]
    }
   ],
   "source": [
    "mobydick = nltk.Text(words)\n",
    "mobydick.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freqs = mobydick.vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probs = nltk.MLEProbDist(freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.003473673313677301"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.prob('whale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.prob('vanillafrog')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### with smoothing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probs_d = nltk.WittenBellProbDist(freqs, freqs.B()+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<WittenBellProbDist based on 260819 samples>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06895579290059115"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this made up word should have the same prob as words with word count 1 in the corpus\n",
    "probs_d.prob('vanillafrog')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### try to make a bigram distr and use it to make a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mobydick_bg = nltk.bigrams(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mobydick_bg_fr = nltk.FreqDist(mobydick_bg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for k, v in mobydick_bg_fr.items():\n",
    "#     print k, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mobydick_bg_fr[(u'white', u'whale')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probs_bg = nltk.MLEProbDist(mobydick_bg_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probs_bg = nltk.WittenBellProbDist(freqs, freqs.B()+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00011885682736620939"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs_bg.prob((u'white', u'whale'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#probs_fq = nltk.ConditionalFreqDist(nltk.bigrams(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# probs_bg = nltk.ConditionalProbDist(probs_fq, nltk.MLEProbDist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MLEProbDist based on 191 samples>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# probs_bg['white']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = \"I would like to buy one can of white\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ConditionalProbDist' object has no attribute 'prob'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-136-6e7560de77df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbigram\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;31m#print bigram, probs_bg.prob(bigram)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mmatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbigram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs_bg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbigram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;31m#WHY does the same bigram (white, whale) appear svrl times, something is strange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m#ed: no makes sense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ConditionalProbDist' object has no attribute 'prob'"
     ]
    }
   ],
   "source": [
    "matches = []\n",
    "for bigram in nltk.bigrams(words):\n",
    "    if bigram[0] == unicode(query[-1]):\n",
    "        #print bigram, probs_bg.prob(bigram)\n",
    "        matches.append((bigram, probs_bg.prob(bigram)))  \n",
    "        #WHY does the same bigram (white, whale) appear svrl times, something is strange\n",
    "        #ed: no makes sense\n",
    "matches = set(matches)\n",
    "matches = sorted(matches, key=lambda x: x[1])\n",
    "matches[:-6:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matches.sort(key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((u'white', u'whale'), 0.00011885682736620939)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most likely query: \n",
      "I would like to buy one can of white whale\n"
     ]
    }
   ],
   "source": [
    "print \"Most likely query: \"\n",
    "print \" \".join(query) + \" \" + matches[-1][0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### repeat the same step with a WittenBell distribution instead and try some other 'queries'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  try and do the same as above with custom rap corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a quick and crude parsing, full of garbage tokens like urls and stuff.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "f = open('ohhla.csv')\n",
    "freader = csv.reader(f)\n",
    "s = str()\n",
    "\n",
    "for ix, song in enumerate(freader):\n",
    "    if ix == 0: continue\n",
    "    s += ' '.join(song)\n",
    "    \n",
    "raptext = nltk.word_tokenize(s)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rapcorpus = nltk.Text(raptext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rap_bgf = nltk.FreqDist(nltk.bigrams(rapcorpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rap_bpr = nltk.WittenBellProbDist(rap_bgf, rap_bgf.B() + 1) #why B+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rap_bgf[('a', 'war')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00016570465906266398"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rap_bpr.prob((('a', 'war')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = \"I would like a\".split()\n",
    "matches = []\n",
    "for bigram in nltk.bigrams(rapcorpus):\n",
    "    if bigram[0] == unicode(query[-1]):\n",
    "#         print bigram, probs_bg.prob(bigram)\n",
    "        matches.append((bigram, rap_bpr.prob(bigram)))  \n",
    "\n",
    "matches.sort(key=lambda x: x[1])\n",
    "matches = sorted(set(matches), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most likely query: \n",
      "I would like a\n",
      "1 : nigga\n",
      "2 : soldier\n",
      "3 : war\n",
      "4 : little\n",
      "5 : million\n"
     ]
    }
   ],
   "source": [
    "print \"Most likely query: \"\n",
    "print ' '.join(query)\n",
    "n = 1\n",
    "for match in matches[:-6:-1]:\n",
    "    print n ,':', match[0][1]\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### try with a trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rap_tgf = nltk.FreqDist(nltk.trigrams(rapcorpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rap_tgf[('like', 'a', 'million')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rap_tpr = nltk.WittenBellProbDist(rap_tgf, rap_tgf.B() + 1) #why B+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.427243379693682e-05"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rap_tpr.prob(('like', 'a', 'million'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rap_tprML = nltk.MLEProbDist(rap_tgf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strange:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4579965533144008"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rap_tpr.prob(('like', 'a', 'millionZZZ'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rap_tprML.prob(('like', 'a', 'millionZZZ'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('like', 'a', 'nine') 0.0\n",
      "('like', 'a', 'ref') 0.0\n",
      "('like', 'a', 'Tec..') 0.0\n",
      "('like', 'a', 'three') 0.0\n",
      "('like', 'a', 'fiend') 0.0\n",
      "('like', 'a', 'well-trained') 0.0\n",
      "('like', 'a', 'pair') 0.0\n",
      "('like', 'a', 'Predator') 0.0\n",
      "('like', 'a', 'willow') 0.0\n",
      "('like', 'a', 'Yokohama') 0.0\n",
      "('like', 'a', 'moped') 0.0\n",
      "('like', 'a', 'twenty') 0.0\n",
      "('like', 'a', 'crab') 0.0\n",
      "('like', 'a', '75') 0.0\n",
      "('like', 'a', 'window') 0.0\n",
      "('like', 'a', 'strainer') 0.0\n",
      "('like', 'a', 'Kirby') 0.0\n",
      "('like', 'a', 'rattle') 0.0\n",
      "('like', 'a', 'million') 0.0\n",
      "('like', 'a', 'cut') 0.0\n",
      "('like', 'a', 'live') 0.0\n"
     ]
    }
   ],
   "source": [
    "query = \"I would like a\".split()\n",
    "matches = []\n",
    "for trigram in nltk.trigrams(rapcorpus):\n",
    "    if trigram[0] == unicode(query[-2]) and trigram[1] == unicode(query[-1]):\n",
    "        print trigram, probs_bg.prob(bigram)\n",
    "        matches.append((trigram, rap_tpr.prob(trigram)))  \n",
    "\n",
    "matches = sorted(set(matches), key=lambda x: x[1])\n",
    "\n",
    "# print \"Most likely query: \"\n",
    "# print ' '.join(query)\n",
    "# n = 1\n",
    "# for match in matches[:-6:-1]:\n",
    "#     print n ,':', match[0][1]\n",
    "#     n += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  ////////////////////////////"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?nltk.NgramTagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Problem where ngram model of language may choose the wrong context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "gutenberg.fileids()\n",
    "# words = gutenberg.words('melville-moby_dick.txt')\n",
    "words = gutenberg.words()\n",
    "\n",
    "gutenberg_bg = nltk.bigrams(words)\n",
    "gutenberg_bg_fr = nltk.FreqDist(gutenberg_bg)\n",
    "probs_bg = nltk.MLEProbDist(gutenberg_bg_fr)\n",
    "\n",
    "query = \"I would like to buy one can of frozen\".split()\n",
    "matches = []\n",
    "for bigram in nltk.bigrams(words):\n",
    "    if bigram[0] == unicode(query[-1]):\n",
    "        #print bigram, probs_bg.prob(bigram)\n",
    "        matches.append((bigram, probs_bg.prob(bigram)))  \n",
    "matches = set(matches)\n",
    "matches = sorted(matches, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((u'frozen', u'maid'), 1.144334096731324e-06),\n",
       " ((u'frozen', u'for'), 1.144334096731324e-06),\n",
       " ((u'frozen', u'into'), 3.8144469891044136e-07),\n",
       " ((u'frozen', u'surface'), 3.8144469891044136e-07),\n",
       " ((u'frozen', u'feet'), 3.8144469891044136e-07)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches[:-6:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('would', 'MD'),\n",
       " ('like', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('buy', 'VB'),\n",
       " ('one', 'CD'),\n",
       " ('can', 'MD'),\n",
       " ('of', 'IN'),\n",
       " ('frozen', 'VBN'),\n",
       " ('princess', 'NN')]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(\"I would like to buy one can of frozen princess\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2621612"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(nltk.bigrams(gutenberg.words())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try the same but in combin. with a pos tagger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(u'frozen', 'VBN'), (u'maid', 'VBD')],\n",
       " [(u'frozen', 'VBN'), (u'for', 'IN')],\n",
       " [(u'frozen', 'VBN'), (u'into', 'IN')],\n",
       " [(u'frozen', 'VBN'), (u'surface', 'NN')],\n",
       " [(u'frozen', 'VBN'), (u'feet', 'NN')]]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map(lambda x: nltk.pos_tag(x[0]), matches[:-6:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tagged candidate sentences:\n",
    "topma = map(lambda x: x[0], matches[:-6:-1])\n",
    "cands = map(lambda x: unicode(' '.join(query)) + ' ' + x[1], topma)\n",
    "tcands = map(lambda x: nltk.pos_tag(nltk.tokenize.word_tokenize(x)), cands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(u'I', 'PRP'),\n",
       "  (u'would', 'MD'),\n",
       "  (u'like', 'VB'),\n",
       "  (u'to', 'TO'),\n",
       "  (u'buy', 'VB'),\n",
       "  (u'one', 'CD'),\n",
       "  (u'can', 'MD'),\n",
       "  (u'of', 'IN'),\n",
       "  (u'frozen', 'VBN'),\n",
       "  (u'maid', 'VBN')],\n",
       " [(u'I', 'PRP'),\n",
       "  (u'would', 'MD'),\n",
       "  (u'like', 'VB'),\n",
       "  (u'to', 'TO'),\n",
       "  (u'buy', 'VB'),\n",
       "  (u'one', 'CD'),\n",
       "  (u'can', 'MD'),\n",
       "  (u'of', 'IN'),\n",
       "  (u'frozen', 'VBN'),\n",
       "  (u'for', 'IN')],\n",
       " [(u'I', 'PRP'),\n",
       "  (u'would', 'MD'),\n",
       "  (u'like', 'VB'),\n",
       "  (u'to', 'TO'),\n",
       "  (u'buy', 'VB'),\n",
       "  (u'one', 'CD'),\n",
       "  (u'can', 'MD'),\n",
       "  (u'of', 'IN'),\n",
       "  (u'frozen', 'VBN'),\n",
       "  (u'into', 'IN')],\n",
       " [(u'I', 'PRP'),\n",
       "  (u'would', 'MD'),\n",
       "  (u'like', 'VB'),\n",
       "  (u'to', 'TO'),\n",
       "  (u'buy', 'VB'),\n",
       "  (u'one', 'CD'),\n",
       "  (u'can', 'MD'),\n",
       "  (u'of', 'IN'),\n",
       "  (u'frozen', 'VBN'),\n",
       "  (u'surface', 'NN')],\n",
       " [(u'I', 'PRP'),\n",
       "  (u'would', 'MD'),\n",
       "  (u'like', 'VB'),\n",
       "  (u'to', 'TO'),\n",
       "  (u'buy', 'VB'),\n",
       "  (u'one', 'CD'),\n",
       "  (u'can', 'MD'),\n",
       "  (u'of', 'IN'),\n",
       "  (u'frozen', 'VBN'),\n",
       "  (u'feet', 'NNS')]]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "?nltk.grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?nltk.parse.MaltParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grammar parser doesn't seem to be what people use, there is no way to specify deterministically a grammar for the entire english language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "possible way to filter the ngram matches: most likely pos tagset continuation, but how exactly.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hmmmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "?nltk.BigramTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training = nltk.corpus.treebank.tagged_sents()[:3522]\n",
    "tagger = nltk.BigramTagger(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  another way to use the nltk api:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  http://www.katrinerk.com/courses/python-worksheets/language-models-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am </s> <s> Sam I am Sam I do not like green eggs and ham </s> <s> I do not like green eggs and ham </s> <s> Sam I am </s> <s> I do not like green eggs and ham </s> <s> I am Sam I am </s> <s>\n",
      "ancient history , and sealed . Recovering the half-year left Helva logically , during the naked eye at first few lines on a verb or more friendly to think of Jack's total darkness . Thus events , by solution in Siddo . `` Well , it wonderful to intercept orbit\n",
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# an nltk.FreqDist() is like a dictionary,\n",
    "# but it is ordered by frequency.\n",
    "# Also, nltk automatically fills the dictionary\n",
    "# with counts when given a list of words.\n",
    "\n",
    "freq_brown = nltk.FreqDist(brown.words())\n",
    "\n",
    "freq_brown.keys()\n",
    "freq_brown.items()[:20]\n",
    "\n",
    "# an nltk.ConditionalFreqDist() counts frequencies of pairs.\n",
    "# When given a list of bigrams, it maps each first word of a bigram\n",
    "# to a FreqDist over the second words of the bigram.\n",
    "\n",
    "cfreq_brown_2gram = nltk.ConditionalFreqDist(nltk.bigrams(brown.words()))\n",
    "\n",
    "# conditions() in a ConditionalFreqDist are like keys()\n",
    "# in a dictionary\n",
    "\n",
    "cfreq_brown_2gram.conditions()\n",
    "\n",
    "# the cfreq_brown_2gram entry for \"my\" is a FreqDist.\n",
    "\n",
    "cfreq_brown_2gram[\"my\"]\n",
    "\n",
    "# here are the words that can follow after \"my\".\n",
    "# We first access the FreqDist associated with \"my\",\n",
    "# then the keys in that FreqDist\n",
    "\n",
    "cfreq_brown_2gram[\"my\"].keys()\n",
    "\n",
    "# here are the 20 most frequent words to come after \"my\", with their frequencies\n",
    "\n",
    "cfreq_brown_2gram[\"my\"].items()[:20]\n",
    "\n",
    "# an nltk.ConditionalProbDist() maps pairs to probabilities.\n",
    "# One way in which we can do this is by using Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "cprob_brown_2gram = nltk.ConditionalProbDist(cfreq_brown_2gram, nltk.MLEProbDist)\n",
    "\n",
    "# This again has conditions() wihch are like dictionary keys\n",
    "\n",
    "cprob_brown_2gram.conditions()\n",
    "\n",
    "# Here is what we find for \"my\": a Maximum Likelihood Estimation-based probability distribution,\n",
    "# as a MLEProbDist object.\n",
    "\n",
    "cprob_brown_2gram[\"my\"]\n",
    "\n",
    "# We can find the words that can come after \"my\" by using the function samples()\n",
    "\n",
    "cprob_brown_2gram[\"my\"].samples()\n",
    "\n",
    "# Here is the probability of a particular pair:\n",
    "\n",
    "cprob_brown_2gram[\"my\"].prob(\"own\")\n",
    "\n",
    "# and we can draw a random word to follow \"my\"\n",
    "# based on the probabilities of the bigrams\n",
    "\n",
    "cprob_brown_2gram[\"my\"].generate()\n",
    "\n",
    "# We can use this to generate text at random\n",
    "# based on a given text of bigrams.\n",
    "# Let's do this for the Sam \"corpus\"\n",
    "\n",
    "corpus = \"\"\"<s> I am Sam </s>\n",
    "<s> Sam I am </s>\n",
    "<s> I do not like green eggs and ham </s>\"\"\"\n",
    "\n",
    "words = corpus.split()\n",
    "cfreq_sam = nltk.ConditionalFreqDist(nltk.bigrams(words))\n",
    "cprob_sam = nltk.ConditionalProbDist(cfreq_sam, nltk.MLEProbDist)\n",
    "\n",
    "word = \"<s>\"\n",
    "for index in range(50):\n",
    "    word = cprob_sam[ word].generate()\n",
    "    print word,\n",
    "print\n",
    "\n",
    "# Not a lot of variety. We need a bigger corpus.\n",
    "# What kind of genres do we have in the Brown corpus?\n",
    "brown.categories()    \n",
    "\n",
    "# Let's try Science Fiction.\n",
    "cfreq_scifi = nltk.ConditionalFreqDist(nltk.bigrams(brown.words(categories = \"science_fiction\")))\n",
    "cprob_scifi = nltk.ConditionalProbDist(cfreq_scifi, nltk.MLEProbDist)\n",
    "\n",
    "word = \"in\"\n",
    "for index in range(50):\n",
    "    word = cprob_scifi[ word ].generate()\n",
    "    print word,\n",
    "print\n",
    "\n",
    "# try this with other Brown corpus categories.\n",
    "\n",
    "# For the nltk.book objects, there is a generate() function.\n",
    "from nltk.book import *\n",
    "# text6.generate()\n",
    "# text7.generate()\n",
    "# text2.generate()\n",
    "\n",
    "# Do you think they used bigrams like we did earlier, or some larger n-grams?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#  ///\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# crap\n",
    "class LangModel:\n",
    "  def __init__(self, order, alpha, sentences):\n",
    "    self.order = order\n",
    "    self.alpha = alpha\n",
    "    if order > 1:\n",
    "      self.backoff = LangModel(order - 1, alpha, sentences)\n",
    "      self.lexicon = None\n",
    "    else:\n",
    "      self.backoff = None\n",
    "      self.n = 0\n",
    "    self.ngramFD = nltk.FreqDist()\n",
    "    lexicon = set()\n",
    "    for sentence in sentences:\n",
    "      words = nltk.word_tokenize(sentence)\n",
    "      wordNGrams = nltk.ngrams(words, order)\n",
    "      for wordNGram in wordNGrams:\n",
    "        #self.ngramFD.inc(wordNGram)\n",
    "        self.ngramFD[wordNGram] += 1\n",
    "        if order == 1:\n",
    "          lexicon.add(wordNGram)\n",
    "          self.n += 1\n",
    "    self.v = len(lexicon)\n",
    "\n",
    "  def logprob(self, ngram):\n",
    "    return math.log(self.prob(ngram))\n",
    "  \n",
    "  def prob(self, ngram):\n",
    "    if self.backoff != None:\n",
    "      freq = self.ngramFD[ngram]\n",
    "      backoffFreq = self.backoff.ngramFD[ngram[1:]]\n",
    "      if freq == 0:\n",
    "        return self.alpha * self.backoff.prob(ngram[1:])\n",
    "      else:\n",
    "        return freq / backoffFreq\n",
    "    else:\n",
    "      return ((self.ngramFD[ngram] + 1) / (self.n + self.v))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# words = nltk.corpus.gutenberg.words('melville-moby_dick.txt')\n",
    "# lm = LangModel(1, 0.8, ' '.join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = [\"In biology, immunity is the state of having sufficient biological defences to avoid infection, disease, or other unwanted biological invasion.\",\n",
    "     \"Naturally acquired immunity occurs through contact with a disease causing agent, when the contact was not deliberate, whereas artificially acquired immunity develops only through deliberate actions such as vaccination.\"]\n",
    "lm = LangModel(2, 0.5, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = nltk.corpus.gutenberg.words('melville-moby_dick.txt')\n",
    "#text = nltk.Text(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
