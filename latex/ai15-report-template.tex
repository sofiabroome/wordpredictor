\documentclass[a4paper,12pt]{article}

\usepackage{url}
\usepackage{epsfig}
\usepackage{graphics}
\usepackage{fancyhdr}
\usepackage{listings}
\usepackage{booktabs}
\usepackage{lastpage}


\graphicspath{{pictures/}}

\title{Word prediction performance of n-gram models applied to essentially different corpora}
\author{\hspace*{-0.5cm}
GROUP 34\\
\begin{tabular}{cccc}
Sofia Broom\'e & Jeremy Krebs & Valentin Geffrier & Erik Fredriksen \\
901210 & 920421 & BIRTHDATE3 & 860328 \\
sbroome@kth.se & jeremyk@kth.se & MAIL3@kth.se & efred@kth.se \\
\includegraphics[width=0.13\linewidth]{Nikkaluokta} & 
\includegraphics[width=0.13\linewidth]{Alan_Turing_photo} & 
\includegraphics[width=0.13\linewidth]{Alan_Turing_photo} & 
\includegraphics[width=0.13\linewidth]{erikpancakes}
\end{tabular}} 
% Normally there will not be any pictures but we want
% these so that we can connect faces to names in the course
% We also want birthdates so that we can tell people with the same
% name apart
\date{}

\pagestyle{fancy}
\setlength{\headheight}{15pt}
\fancyhf{}
\cfoot{\thepage / \pageref{LastPage}}
\lhead{DD2380 ai15} % DO NOT REMOVE!!!!
\rhead{S. Broom\'e, J. Krebs, V. Geffrier, E. Fredriksen} %% UPDATE WITH YOUR NAMES

\begin{document}

\maketitle
\thispagestyle{fancy}

\begin{abstract}
An investigation of how basic n-gram models of our own implementation can be improved by adding smoothing techniques and grammar constraints. Tests have been performed on corpora of varying character.
\end{abstract}



\clearpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{NOTE}
\begin{itemize}

\item \textit{Introduction, Related Works, Experimental Results, Discussions, Summary} are sections that MUST be contained.

\item The section \textit{Contributions} is a place to express any difference in contributions. The default assumption is that you all agree that all of you had an equal part to play in the project.

\item We suggest that you try to write this as scientifically as possible and not simply like a project report. Good Luck!

\end{itemize}
\section{Introduction}
\label{sec:intro}

Being able to dissect, classify, analyze and reproduce language is a highly relevant task for various fields. In the realm of artificial intelligence, we want to give language to our agents by means of communicating with them. When we deal with natural language processing we say that we make language models. Seen as there is no finite set of rules that can describe, say, the entire English language in a complete sense, for pragmatic reasons our best option seems to be basing our models on probabilistic observations - regardless of Noam Chomsky's contempt\cite{JurafskyBook} for the notion of probability of a sentence.

At the foundation of every language model that wants to predict words is the concept of n-grams, a method based on probabilistic distributions over length n combinations of subsequent words. An n-gram is a Markov chain of degree n-1. This quite simple construct can capture many patterns in sentences. Even though it doesn't consider grammar explicitly, grammar will inevitably be built in. For instance, an adjective will in many cases be followed by a noun, or a pronoun by a verb, and thus a bigram composed of those two grammatical types in the mentioned order will score high in probability. 

An n-gram gives us context for words, albeit not the full one. Gao and Suzuki\cite{gao2004long} explore long distance dependency for words through word clusters and the linguistically motivated {\it function word skipping} method where function words such as "has", "a", "in", "and", "the", etc, are skipped in favor of more significant words, called head words. In our experiments however, we will not delve further into this subject.

N-grams can also be used in a meta-sense - for instance it's common for part-of-speech-taggers to use n-gram models where they tag the current word based on the last word's tag.

There are some practical issues with the classical n-gram model. What do we do with the n-grams that aren't in our training set and thus have zero probability assigned? This is where techniques of so called smoothing comes in so that our model doesn't fail upon encountering a previously unseen word in the test set. And in case we are dealing with a higher-order n-gram and we find it has no probability mass , we might want to "back off" from the higher order and estimate the probability for a conditioned unigram, meaning we temporarily look at a smaller portion of a word's history.

Furthermore, what kinds of test sets does our training set allow us to perform well on? One should train on a  corpus which is representative of the domain of the intended use. And what happens to our model when we apply it to languages with a higher degree of inflection like Swedish, Basque or German?

From the above examples we see that in many cases just using the n-gram model in itself will not suffice. Over the years, researchers in natural language processing have added a lot of tweaks to the original idea such as linear combinations of n-grams, cache language models, LSA-based language models and maximum entropy models, to name a few.

In what follows we will explore n-gram models of varying degrees on dito corpora and grammar to see which results are obtained under which circumstances.

\subsection{Related works}
We began our exploring of natural language processing through the Natural Language Toolkit website and the accompanying book, Natural Language Processing with Python. The theory to accompany the more practical methods from NLTK was mainly obtained from Daniel Jurafsky and James H. Martin's 1999 book Speech and Language processing \ref{JurafskyBook}. Daniel Jurafsky also has made a series of video lectures on the subject for a Coursera online course. From \ref{brown1992} and \ref{garay2006text} we got a sense of the challenges of extending the concept of n-grams, as well as for modern applications of text prediction.

\subsection{Outline}
The lion's share of the report consists of three sections, hereunder, concerning respectively n-gram models, n-gram models with smoothing and n-gram models with grammar constraints, where each of them contains some theory, accounts of our related experiments and discussions of possible issues. In the end, the results of our work are presented with a subsequent summary and conclusion.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%   NGRAM MODELS   %%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{N-gram models}
\label{sec:ngram}

\subsection{Theory}
The first mathematical tool needed in all natural language processing experiment is n-gram models. Speech taggers, smoothing and other methods may not be implemented at first, but n-gram models are needed to predict the next likely words of a sentence.

These models are quite easy to understand: if n is a fixed integer, a n-gram model works as a Markov chain to predict the next word. A model is trained on a corpora C, for instance a book or a set of books, so that the probability of each n-gram in the language is learned by the model. The bigger the corpora the more accurate and reliable will be the model as it will have more data to compute probabilities of sequences. More precisely, the probability $p$ that the word $w_n$ follows the group of words $w_1 w_2 ... w_{n-1}$ is given by the following formula:

$$ p = p(w_n | w_1 .. w_{n-1}) = \frac{|{(w_1, .., w_{n-1}, w_n) \in C}|}{|{(w_1, .., w_{n-1}, x) \in C}|} $$

With this formula, it is possible to know the more likely next word of a sentence, but also to generate the end of the sentence, each new word selected using the probability distribution. Therefore, a n-gram which appeared a lot in a corpora is more likely to appear in the sentence generator. However, this also tells us that heuristically, we should expect different results from one corpora to another. For instance using corpora from Shakespeare, the generated sentences are likely to be more erratic than with a novel since Shakespeare's syntax and grammar are more complicated. For modernist poetry like Whitman's Leaves of Grass (corpus \# 4) it turns out to work quite well with a basic n-gram model, since the somewhat abstract resulting sentences fit with the textual situation and since the verse structure isn't as demanding as in shakespearian texts. Poetry has higher context satisfaction for a crude n-gram model.

\subsection{Experiments}
\subsubsection{Parameter $n$}
	The first experiment we did was to try to understand the influence of n in our word predictor and how we could tune this parameter. In these experiments, only the corpora and n is changed - no parts of speech tagger or smoothing have been used. The table \ref{tab:corpus2} shows n-gram predictions of the end of the sentence "Alice was looking for" for $ n\in(2,4) $.
	
We should note that a 1-gram model is not a Markov model. It just predicts a word according to its frequency in a book. As punctuation symbols are considered as words in this corpora, that explains why it predicts so many commas and apostrophes. 

A 2-gram model will only predict a word according to the immediately preceding word in the sequence. This explains why after words like "the" and "a" there are often adjectives or nouns. However, the prediction is still not perfect because of the short reach of a 2-gram model.

With 3-gram models and 4-gram models one can see that the sentences are a bit more grammatically correct but there is still some issues since the English grammar and syntax are not used in these models. Another thing with 4-gram models is that we can see the predictions are quite close to each other for different attempts. This is because the more words are fixed, the less freedom there is for the next word: there are fewer bigrams starting with "for" than 4-grams starting with "was looking for" in the corpora.
	
If n is too low, the predictor will be inferior because one or two words might not be enough to predict a likely next word. However if n is too big, the corpora needs to be really extensive as well and contain enough different n-grams so that the predictor is more diversified.

\subsubsection{Example results}
\begin{table}
\begin{center}
\begin{tabular}{|c|l|c|c|}
\hline
\# & Tested corpora & Year & Nr. of words\\ \hline
1 & Selected poems by William Blake & 1789 & 8354 \\ \hline
2 & Alice's Adventures in Wonderland by Lewis Carroll & 1865 & 34110 \\ \hline
3 & The Bible & & 100000 (1010654) \\ \hline
4 & Leaves of Grass by Walt Whitman & 1891 & 154883\\ \hline
5 & Moby Dick by Herman Melville & 1851 & 260819\\ \hline
\end{tabular}
\caption{The corpora tested in the n-gram experiments.}
\label{tab:corplist}
\end{center}
\end{table}

\begin{table}
%\begin{center}
\begin{tabular}{| l |c|}
\hline
Query followed by 10 generated words & $n$ \\ \hline
"Infant smiles are his own . Gone was thy Maker lay . Pretty joy to " & 2\\ \hline
"Infant smiles are his own destruction ? Can delight . Folly is dwelling too ?"& 2 \\ \hline
"Infant smiles are his own smiles on my foe outstretched beneath the skies ; And" & 2 \\ \hline
"Infant smiles are his own grave plot she in darkness plough ? Can a threat" & 2 \\ \hline

"Infant smiles are his own smiles ; Heaven and earth to peace beguiles . DIVINE " & 3 \\ \hline
"Infant smiles are his own smiles ; Heaven and earth to peace beguiles . DIVINE" & 3 \\ \hline
"Infant smiles are his own smiles ; Heaven and earth to peace , and fled " & 3 \\ \hline
"Infant smiles are his own smiles ; Heaven and earth to peace beguiles . DIVINE" & 3 \\ \hline

"Infant smiles are his own smiles ; Heaven and earth to peace beguiles . DIVINE" & 4 \\ \hline
"Infant smiles are his own smiles ; Heaven and earth to peace beguiles . DIVINE" & 4 \\ \hline
"Infant smiles are his own smiles ; Heaven and earth to peace beguiles . DIVINE" & 4 \\ \hline

\end{tabular}
\caption{ Corpus 1}
\label{tab:corpus1}
%\end{center}
\end{table}

\begin{table}
%\begin{center}
\begin{tabular}{| l |c|}
\hline
Query followed by 10 generated words & $n$ \\ \hline
"Alice was looking for a very neatly spread his belt and began . ' " & 2\\ \hline
"Alice was looking for eggs , I can kick a little bottle that stood "& 3 \\ \hline
"Alice was looking for the fan and a pair of white kid gloves , " & 4 \\ \hline
\end{tabular}
\caption{Corpus 2}
\label{tab:corpus2}
%\end{center}
\end{table}
\begin{table}
%\begin{center}
\begin{tabular}{| l |c|}
\hline
Query followed by 10 generated words & $n$ \\ \hline
"And they were both naked unto Pharaoh ' s milk , and shut him . " & 2\\ \hline
"And they were both naked ? 26 And Miriam the flesh from thence did Sarah"& 2 \\ \hline
"And they were both naked ? what hast done in the best of the north" & 2 \\ \hline
"And they were both naked ; and daubed it without blemish unto him : 13" & 2 \\ \hline

"And they were both naked , the people , and Calneh , in the coupling " & 3 \\ \hline
"And they were both naked , the face of the burnt offering , even the" & 3 \\ \hline
"And they were both naked , the children of Israel , and the sheep ." & 3 \\ \hline
"And they were both naked , the camels had done to him as before ." & 3 \\ \hline
"And they were both naked , the God of thy servants it be for a" & 3 \\ \hline

"And they were both naked , the man that brought us up out of the" & 4 \\ \hline
"And they were both naked , the man is become as one of the Hebrews" & 4 \\ \hline
"And they were both naked , the man is become as one of them opened" & 4 \\ \hline
"And they were both naked , the man that brought us up out of that" & 4 \\ \hline
"And they were both naked , the man and his household came with Jacob ." & 4 \\ \hline

\end{tabular}
\caption{ Corpus 3}
\label{tab:corpus3}
%\end{center}
\end{table}

\begin{table}
%\begin{center}
\begin{tabular}{| l |c|}
\hline
Query followed by 10 generated words & $n$ \\ \hline
"The delicious singing of the mother ' d ship - clad soldiers of ooze and what" & 2\\ \hline
"The delicious singing of the mother ' d eve delicious word , Elements merge with the"& 2 \\ \hline
"The delicious singing of the mother ' d back on the midst of Nature your tongue" & 2 \\ \hline
"The delicious singing of the mother kisses of the clasp me shall cover ' s funeral" & 2 \\ \hline
"The delicious singing of the mother told - lung ' d every blow through the same" & 2 \\ \hline

"The delicious singing of the mother sleeps with at night , The singers of old or" & 3 \\ \hline
"The delicious singing of the mother misused by her children , resolute , under the sun" & 3 \\ \hline
"The delicious singing of the mother never turning her vigilant eyes ,) Calmly a lady '" & 3 \\ \hline
"The delicious singing of the mother of many nations , the shelves are crowded with perfumes" & 3 \\ \hline

"The delicious singing of the mother shines on the white wrist of the daughter , The" & 4 \\ \hline
"The delicious singing of the mother , the Mississippi flows , Of mighty inland cities yet" & 4 \\ \hline
"The delicious singing of the mother of many children , These clamors wild to a race" & 4 \\ \hline
"The delicious singing of the mother , or of me ; Of their languages , governments" & 4 \\ \hline
"The delicious singing of the mother shines on the white wrist of the daughter , The" & 4 \\ \hline

"Sing on there in the swamp - congratulatory signs and dead . I shall never refuses" & 2\\ \hline
"Sing on there in the swamp in songs , English war , Sunlight by the Cascade"& 2 \\ \hline
"Sing on there in the swamp - dug in disgrace to give an open air I" & 2 \\ \hline
"Sing on there in the swamp , busier sphere more in granite walls of the West" & 2 \\ \hline
"Sing on there in the swamp in vain the country ; You past .) Now I" & 2 \\ \hline

"Sing on there in the swamp in the houses are alive with people , I pause" & 3 \\ \hline
"Sing on there in the swamp in secluded recesses , From the head of the real" & 3 \\ \hline
"Sing on there in the swamp in the ranks marching , on I go , I'" & 3 \\ \hline
"Sing on there in the swamp - perfume , with brow elate and governing hand ." & 3 \\ \hline

"Sing on there in the swamp , O singer bashful and tender , I hear in" & 4 \\ \hline
"Sing on there in the swamp , O singer bashful and tender , I hear the" & 4 \\ \hline
"Sing on there in the swamp , O singer bashful and tender , I hear the" & 4 \\ \hline
"Sing on there in the swamp , O singer bashful and tender , I hear in" & 4 \\ \hline

\end{tabular}
\caption{ Corpus 4}
\label{tab:corpus4}
%\end{center}
\end{table}

\begin{table}
%\begin{center}
\begin{tabular}{| l |c|}
\hline
Query followed by 10 generated words & $n$ \\ \hline
"However , there is a whale - fleet of this , and the two" & 2\\ \hline
"However , there is no place to live in the captain ' s a"& 3 \\ \hline
"However , there is no telling . But a day or two previous ," & 4 \\ \hline
"I think that you must have been offered a whale , when the long" & 2\\ \hline
"I think that you do yours in approved state stocks bringing in good time"& 3 \\ \hline
"I think that you really perceived drops of moisture in the spout - hole" & 4 \\ \hline
\end{tabular}
\caption{Corpus 5}
\label{tab:corpus5}
%\end{center}
\end{table}

\subsection{Issues}

\section{N-gram models with smoothing}

\subsubsection{Initial practical problems: an SRILM excursion}
Building a full-fledged language model in Python is not a trivial endeavor (this could be a project in itself). We thought NLTK provided the facilities to accomplish this, but it turned out this module had been deprecated due to bugs. 

SRI provides an extensive framework for just this task trough SRILM (this framework is also evidently widely adopted - the introductory SRILM article by Andreas Stolcke has over 3 000 Google Scholar citations). 

SRILM itself is a library of C++ classes, but contains ready-made command line tools. For example, invoking "ngram-count" on the Moby Dick corpus from NLTK (where corpus.txt has been parsed to match SRILMs format: one sentence per line with tokens separated by whitespace) produces an ngram language model of this text, with Good-Turing discounting and Katz backoff for smoothing.

Doing actual predictions with this framework proved to be a bit harder, we found a Python wrapper (pysrilm) that was supposed to provide this functionality, however, it was a Cython extension that needed to be compiled and linked against the SRILIM library. This did not work, so we returned to our own Python n-gram model to implement a smoothing method ourselves.

\label{sec:ngramsmoothing}

\subsection{Theory}
	The issue behind simple n-gram models is that the maximum likelihood estimate only consider n-grams in the corpora. That is, many n-grams are not considered by the model to have a probability strictly positive because there is sparsity in the data. However in most cases one does not want to give a probability of 0 to an unseen event seen as an event that did not occur in the training data could occur in the test data. Smoothing methods are used to adapt the probability distribution so that unseen events have a probability greater than 0. There are different ways of smoothing and we experimented with mainly three methods implemented in $nltk$:
	
	\begin{description}
		\item[Witten-Bell:] This method gives the same probability mass to the unseen events as the total probability mass of the events seen exactly once in the corpora. As there are usually a lot more unseen events than events that occured once, the model seems reliable because it will give a low probability to the unseen event. A given parameter is the number of total events, seen or unseen. If there are $m$ unique words in the corpora and we are considering n-grams, the parameter will not necessarily be as big as $m^n$ since the corpora could be lacking words that we want to add manually, or we can ignore some n-grams because they are not valid grammatically.
		\item[Lidstone:] This method simply adds a value $\gamma$ to the frequency of all events. It then computes the basic maximum-likelihood probability based on those new frequencies. This parameter, is commonly set to around 0.5 but can be tuned. With that value, the weight of an unseen event will be a third of the weight of an event that occured once. We can note that this method derives from the Laplace smoothing method which is when $\gamma = 1$.
		\item[Good-Turing:] A more complicated method that gives the same probability to the events that occured $n$ times as to the events that occured $n+1$ times. With high values of $n$ it can happen that no event occured that many times. For these events, an estimation is done. In NLTK, the number of events that occured $n$ times is estimated to be $N(n) = an^b$ where $a$ and $b$ are estimated using linear regression on the logarithm of the equation: $log\ N(n) = log\ a + b\ log\ n$.
	\end{description}
	
	\subsection{Experiments}
	
	In the following experiments, we tried to point out how smoothing methods could change the probability distribution of the n-grams for our predictor. We also compared the sentences with or without smoothing, but comparing the probability distribution makes more sense to understand how smoothing methods work. Three n-grams of different frequencies in the corpora served as illustrative examples for how their (respective) were changed with smoothing.
	
\subsubsection{The probability distribution}
	In figure \ref{fig:smoothingdist}, we tried to see what impact the three above techniques had on the probability distribution of the 2-grams and 4-grams for the corpora "Alice in Wonderland". The n-grams with low frequency are not considered here as their probability is roughly the same after smoothing. As we could expect, the blue (smoothed) graphs are generally lower than the red (unsmoothed) because probability weight is taken from seen events to unseen events. The mass distribution however varies between the different methods; the Lidstone probability for instance does not take the same weight from all n-grams. With our parameters, Good Turing doesn't change much of the probability distribution from the unsmoothed case, from which we conclude that the tuning of the parameters is important. We should also take the size of the corpora into account as the ratio between the number of unseen events and seen events has a crucial effect in the computation of probabilites.

\begin{figure}
	\label{fig:smoothingdist}
	\begin{tabular}{cc}
		\includegraphics[width=0.52\linewidth]{2_Lidstone} & 
		\includegraphics[width=0.52\linewidth]{4_Lidstone} \\
		\includegraphics[width=0.52\linewidth]{2_WittenBell} & 
		\includegraphics[width=0.52\linewidth]{4_WittenBell} \\
		\includegraphics[width=0.52\linewidth]{2_Turing} & 
		\includegraphics[width=0.52\linewidth]{4_Turing} \\
	\end{tabular}
	\caption{Changes of the probability distribution for different smoothing methods}
\end{figure}

\subsubsection{Specific example}
	Table \ref{tab:smoothingprobs} shows the probability and probability ratio change of three bigrams from the same corpora: (she, was) with a frequency of 49, (she, is) of count 3 and the unseen (she, blue). Zero probabilities as removed as intended. We can however see the ratio of the probabilities between (she, was) and (she, is) change differently depending on the method. If Witten-Bell does not change it, Lidstone and Turing does as they do not allocate the same weight percentage of each seen events to the unseen ones.


\begin{table}[!h]
\centering
\caption{Data for different smoothing techniques}
\label{tab:smoothingprobs}
\begin{tabular}{@{}lllll@{}}
\toprule
Smoothing:     & None      & SGT       & WB        & LSPB 0.5   \\ \midrule
P(she, is)     & 8.795e-05 & 6.072e-05 & 6.025e-05 & 8.343e-05  \\
P(she, was)    & 0.001437  & 0.001396  & 0.0009841 & 0.001180   \\
P(she, blue)   & 0.000     & 3.701e-08 & 3.469e-08 & 1.192e-05  \\
Ratio was/is   & 16.33     & 22.99     & 16.33     & 14.14      \\
Ratio was/blue &           & 37710     & 28370     & 99.00      \\ \bottomrule
\end{tabular}
\end{table}

\subsection{Issues}
	The last example can show how careful one needs to be when it comes to smoothing. If the parameter is badly tuned, the probability of an unseen event such as (she, blue) can be high regardless of its obvious grammatical problems. We can thus conclude that n-gram models simply in combination with smoothing technics cannot grasp the complex syntax, grammar and structure of language: we need grammar constraints if we want to improve.


\section{N-gram models with grammar constraints}
\label{sec:ngramgrammar}

\subsection{Theory}

The standard approach to grammar in language modeling is to tag every word in a corpus with so called parts-of-speech-tags. This is challenging since one word can have varying grammatical roles and semantic meaning depending on its syntactic location. Different tagsets can be used depending on how fine grained you want your model to be, but broadly speaking they can be divided into two categories: rule-based taggers and stochastic taggers. Rule-based taggers are decided according to a set of explicitly written rules that were made to dodge tagging ambiguities. Stochastic taggers on the other hand can function like decision trees orientating through probabilistic features or Hidden markov models where for instance an applied Viterbi algorithm can choose the word-and-tag-path with the highest likelihood. The use of HMMs when working with POS-tagging started in the 1980s. 

\subsection{Experiments}

We have observed that sometimes our n-gram model gives us an answer that doesn?t fit in syntactically. Because of this we decided to use the nature of each word to perfect our predictions. We tried a simple approach: first, we created a new text from all the parts-of-speech-tags (POS-tags) associated to the original words of the text. Then, we trained an n-gram model on this text. Here, n can be higher than when we work with normal corpora because a lot of completely different word sequences can be hidden behind the same POS-tag sequence. For example, we have one particular sequence to complete and n is the longest number for an n-gram model we can accept if we want to find word sequences of length n on our corpora where the first n-1 are the last from our word sequence.

But, if we only consider the tags associated to the words, we can build an m-gram model where m is higher than n and it gives the probability of having one POS tag after the tags representing our word sequence, and it is much more likely to have this kind of tags sequence even if we they do not represent the same words sequence. It means that we have more information about the probability of the tag following our sequence. We decided to multiply the probability of an n-gram word sequence with the probability of the m-gram tag sequence where the last tag is the tag representing the last word of the words sequence. This method changes the original results and makes some sequences more probable than others with the grammatical information we have.

\begin{table}[]
\centering
\caption{Words generated with grammar constraints}
\label{grammarsent}
\begin{tabular}{| l |c|}
\hline
Query followed by 10 generated words                                   & n \\ \hline
Alice was looking for some curiosity . Then the Rabbit . CHAPTER IX .  & 2 \\ \hline
Alice was looking for the voice behind a sulky tone , twinkle --"' and & 2 \\ \hline
Alice was looking for a bat , that : and take the earth .              & 2 \\ \hline
Alice was looking for some time to be in a sort of the book            & 2 \\ \hline
Alice was looking for eggs , as well go in at the Duchess ;            & 3 \\ \hline
Alice was looking for eggs , as the whole court was a dispute going    & 3 \\ \hline
Alice was looking for eggs , as the game . The judge , would           & 3 \\ \hline
Alice was looking for the Duchess began in a day at school , too       & 3 \\ \hline
Alice was looking for the garden : the others looked round also , and  & 3 \\ \hline
Alice was looking for the fan and gloves , and , as the game           & 4 \\ \hline
Alice was looking for the fan and a pair of gloves and a fan           & 4 \\ \hline
Alice was looking for the fan and a pair of the gloves , and           & 4 \\ \hline
Alice was looking for the fan and the pair of white kid gloves in      & 4 \\ \hline
Alice was looking for the fan and the pair of white kid gloves while   & 4 \\ \hline
Alice was looking for the fan and the pair of white kid gloves ,       & 4 \\ \hline
\end{tabular}
\end{table}

\subsection{Issues}

Without smoothing, we can have the same problem than before where every possible tags sequence according to the possible words Ngrams, have a probability equal to zero. To resolve this, we can use the same smoothing that we used for words and still get results. Our method does not rely on a mathematical explanation but it still allows us to take into consideration the grammar, at a deeper level than for words, at the only condition that we use corpora which have the same grammar type as the text we want to complete, to train our Ngrams model. Indeed, we know that the syntax can be completely different in poetry, Shakespeare works or novels. It was already true when we did not use grammar but it becomes absolutely necessary here or the grammar prediction would just make the performance of our results decrease.
Using parse trees could be an improvement to model grammatical sentence structure, as it is more precise to use a more detailed context than only POS tags.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Summary and Conclusions}
\label{sec:summary}

N-gram models need additional layers consisting of smoothing techniques and grammar constraints to be more credible as word predictors.

The goal of this project was to predict word to complete a word sequence. Having gone from simple bigram models to more complex n-gram models using different smoothing methods and grammar constraints, we have been able to improve the performances of our predictor on different types of corpora.

We also realized that the performance of our system not only relies on the length of our n-gram model but also on its customization with smoothing, grammar constraints, the size of the training corpora and its similarity to our input.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Contributions}
\label{sec:contributions}
We the members of project group34 unanimously declare that 
we have all equally contributed toward the completion of this
project. 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\bibliographystyle{plain}
\bibliography{reflist}



\end{document}
