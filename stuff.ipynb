{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Once', 'upon', 'time', 'there', 'was', 'a', 'hippo', 'named', 'Alfred', '.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = nltk.tokenize.word_tokenize(\"Once upon time there was a hippo named Alfred.\")\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Once', 'RB'),\n",
       " ('upon', 'IN'),\n",
       " ('time', 'NN'),\n",
       " ('there', 'EX'),\n",
       " ('was', 'VBD'),\n",
       " ('a', 'DT'),\n",
       " ('hippo', 'NN'),\n",
       " ('named', 'VBN'),\n",
       " ('Alfred', 'NNP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hey', 'PRP'),\n",
       " (',', ','),\n",
       " ('can', 'MD'),\n",
       " ('I', 'PRP'),\n",
       " ('have', 'VB'),\n",
       " ('that', 'IN'),\n",
       " ('can', 'MD'),\n",
       " ('of', 'IN'),\n",
       " ('olives', 'NNS'),\n",
       " ('?', '.')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tagger gets confused\n",
    "text = nltk.tokenize.word_tokenize(\"Hey, can I have that can of olives?\")\n",
    "#text = nltk.tokenize.word_tokenize(\"Can I have that can of olives?\")\n",
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can: 94\n",
      "could: 87\n",
      "may: 93\n",
      "might: 38\n",
      "must: 53\n",
      "will: 389\n",
      "butter: 2\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "news_text = brown.words(categories='news')\n",
    "fdit = nltk.FreqDist(w.lower() for w in news_text)\n",
    "modals = ['can', 'could', 'may', 'might', 'must', 'will', 'butter']\n",
    "for m in modals:\n",
    "    print m + ':', fdit[m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Once', 'upon'),\n",
       " ('upon', 'a'),\n",
       " ('a', 'time'),\n",
       " ('time', 'there'),\n",
       " ('there', 'was'),\n",
       " ('was', 'a'),\n",
       " ('a', 'hippo'),\n",
       " ('hippo', 'named'),\n",
       " ('named', 'alfed')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"Once upon a time there was a hippo named alfed\".split()\n",
    "list(nltk.bigrams(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## some stuff from MAS and the nltk intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'BROWN CORPUS\\n\\nA Standard Corpus of Present-Day Edited American\\nEnglish, for use with Digital Computers.\\n\\nby W. N. Francis and H. Kucera (1964)\\nDepartment of Linguistics, Brown University\\nProvidence, Rhode Island, USA\\n\\nRevised 1971, Revised and Amplified 1979\\n\\nhttp://www.hit.uib.no/icame/brown/bcm.html\\n\\nDistributed with the permission of the copyright holder,\\nredistribution permitted.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.readme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'[', u'Moby', u'Dick', u'by', u'Herman', u'Melville', ...]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "gutenberg.fileids()\n",
    "words = gutenberg.words('melville-moby_dick.txt')\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sperm Whale; Moby Dick; White Whale; old man; Captain Ahab; sperm\n",
      "whale; Right Whale; Captain Peleg; New Bedford; Cape Horn; cried Ahab;\n",
      "years ago; lower jaw; never mind; Father Mapple; cried Stubb; chief\n",
      "mate; white whale; ivory leg; one hand\n"
     ]
    }
   ],
   "source": [
    "mobydick = nltk.Text(words)\n",
    "mobydick.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freqs = mobydick.vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probs = nltk.MLEProbDist(freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.003473673313677301"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.prob('whale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs.prob('vanillafrog')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### with smoothing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probs_d = nltk.WittenBellProbDist(freqs, freqs.B()+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<WittenBellProbDist based on 260819 samples>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06895579290059115"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this made up word should have the same prob as words with word count 1 in the corpus\n",
    "probs_d.prob('vanillafrog')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### try to make a bigram distr and use it to make a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mobydick_bg = nltk.bigrams(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mobydick_bg_fr = nltk.FreqDist(mobydick_bg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for k, v in mobydick_bg_fr.items():\n",
    "#     print k, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mobydick_bg_fr[(u'white', u'whale')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probs_bg = nltk.MLEProbDist(mobydick_bg_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00011885682736620939"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs_bg.prob((u'white', u'whale'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = \"I would like to buy one can of white\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matches = []\n",
    "for bigram in nltk.bigrams(words):\n",
    "    if bigram[0] == unicode(query[-1]):\n",
    "        #print bigram, probs_bg.prob(bigram)\n",
    "        matches.append((bigram, probs_bg.prob(bigram)))  \n",
    "        #WHY does the same bigram (white, whale) appear svrl times, something is strange\n",
    "        #ed: no makes sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "matches.sort(key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((u'white', u'whale'), 0.00011885682736620939)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most likely query: \n",
      "I would like to buy one can of white whale\n"
     ]
    }
   ],
   "source": [
    "print \"Most likely query: \"\n",
    "print \" \".join(query) + \" \" + matches[-1][0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### repeat the same step with a WittenBell distribution instead and try some other 'queries'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  try and do the same as above with custom rap corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a quick and crude parsing, full of garbage tokens like urls and stuff.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "f = open('ohhla.csv')\n",
    "freader = csv.reader(f)\n",
    "s = str()\n",
    "\n",
    "for ix, song in enumerate(freader):\n",
    "    if ix == 0: continue\n",
    "    s += ' '.join(song)\n",
    "    \n",
    "raptext = nltk.word_tokenize(s)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rapcorpus = nltk.Text(raptext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rap_bgf = nltk.FreqDist(nltk.bigrams(rapcorpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rap_bpr = nltk.WittenBellProbDist(rap_bgf, rap_bgf.B() + 1) #why B+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rap_bgf[('a', 'war')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00016570465906266398"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rap_bpr.prob((('a', 'war')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query = \"I would like a\".split()\n",
    "matches = []\n",
    "for bigram in nltk.bigrams(rapcorpus):\n",
    "    if bigram[0] == unicode(query[-1]):\n",
    "#         print bigram, probs_bg.prob(bigram)\n",
    "        matches.append((bigram, rap_bpr.prob(bigram)))  \n",
    "\n",
    "matches.sort(key=lambda x: x[1])\n",
    "matches = sorted(set(matches), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most likely query: \n",
      "I would like a\n",
      "1 : nigga\n",
      "2 : soldier\n",
      "3 : war\n",
      "4 : little\n",
      "5 : million\n"
     ]
    }
   ],
   "source": [
    "print \"Most likely query: \"\n",
    "print ' '.join(query)\n",
    "n = 1\n",
    "for match in matches[:-6:-1]:\n",
    "    print n ,':', match[0][1]\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### try with a trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rap_tgf = nltk.FreqDist(nltk.trigrams(rapcorpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rap_tgf[('like', 'a', 'million')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rap_tpr = nltk.WittenBellProbDist(rap_tgf, rap_tgf.B() + 1) #why B+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.427243379693682e-05"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rap_tpr.prob(('like', 'a', 'million'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rap_tprML = nltk.MLEProbDist(rap_tgf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strange:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4579965533144008"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rap_tpr.prob(('like', 'a', 'millionZZZ'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rap_tprML.prob(('like', 'a', 'millionZZZ'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('like', 'a', 'nine') 0.0\n",
      "('like', 'a', 'ref') 0.0\n",
      "('like', 'a', 'Tec..') 0.0\n",
      "('like', 'a', 'three') 0.0\n",
      "('like', 'a', 'fiend') 0.0\n",
      "('like', 'a', 'well-trained') 0.0\n",
      "('like', 'a', 'pair') 0.0\n",
      "('like', 'a', 'Predator') 0.0\n",
      "('like', 'a', 'willow') 0.0\n",
      "('like', 'a', 'Yokohama') 0.0\n",
      "('like', 'a', 'moped') 0.0\n",
      "('like', 'a', 'twenty') 0.0\n",
      "('like', 'a', 'crab') 0.0\n",
      "('like', 'a', '75') 0.0\n",
      "('like', 'a', 'window') 0.0\n",
      "('like', 'a', 'strainer') 0.0\n",
      "('like', 'a', 'Kirby') 0.0\n",
      "('like', 'a', 'rattle') 0.0\n",
      "('like', 'a', 'million') 0.0\n",
      "('like', 'a', 'cut') 0.0\n",
      "('like', 'a', 'live') 0.0\n"
     ]
    }
   ],
   "source": [
    "query = \"I would like a\".split()\n",
    "matches = []\n",
    "for trigram in nltk.trigrams(rapcorpus):\n",
    "    if trigram[0] == unicode(query[-2]) and trigram[1] == unicode(query[-1]):\n",
    "        print trigram, probs_bg.prob(bigram)\n",
    "        matches.append((trigram, rap_tpr.prob(trigram)))  \n",
    "\n",
    "matches = sorted(set(matches), key=lambda x: x[1])\n",
    "\n",
    "# print \"Most likely query: \"\n",
    "# print ' '.join(query)\n",
    "# n = 1\n",
    "# for match in matches[:-6:-1]:\n",
    "#     print n ,':', match[0][1]\n",
    "#     n += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  ////////////////////////////"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?nltk.NgramTagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Problem where ngram model of language may choose the wrong context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "gutenberg.fileids()\n",
    "# words = gutenberg.words('melville-moby_dick.txt')\n",
    "words = gutenberg.words()\n",
    "\n",
    "gutenberg_bg = nltk.bigrams(words)\n",
    "gutenberg_bg_fr = nltk.FreqDist(gutenberg_bg)\n",
    "probs_bg = nltk.MLEProbDist(gutenberg_bg_fr)\n",
    "\n",
    "query = \"I would like to buy one can of frozen\".split()\n",
    "matches = []\n",
    "for bigram in nltk.bigrams(words):\n",
    "    if bigram[0] == unicode(query[-1]):\n",
    "        #print bigram, probs_bg.prob(bigram)\n",
    "        matches.append((bigram, probs_bg.prob(bigram)))  \n",
    "matches = set(matches)\n",
    "matches = sorted(matches, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((u'frozen', u'maid'), 1.144334096731324e-06),\n",
       " ((u'frozen', u'for'), 1.144334096731324e-06),\n",
       " ((u'frozen', u'into'), 3.8144469891044136e-07),\n",
       " ((u'frozen', u'surface'), 3.8144469891044136e-07),\n",
       " ((u'frozen', u'feet'), 3.8144469891044136e-07)]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches[:-6:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('would', 'MD'),\n",
       " ('like', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('buy', 'VB'),\n",
       " ('one', 'CD'),\n",
       " ('can', 'MD'),\n",
       " ('of', 'IN'),\n",
       " ('frozen', 'VBN'),\n",
       " ('princess', 'NN')]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(\"I would like to buy one can of frozen princess\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2621612"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(nltk.bigrams(gutenberg.words())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try the same but in combin. with a pos tagger:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(u'frozen', 'VBN'), (u'maid', 'VBD')],\n",
       " [(u'frozen', 'VBN'), (u'for', 'IN')],\n",
       " [(u'frozen', 'VBN'), (u'into', 'IN')],\n",
       " [(u'frozen', 'VBN'), (u'surface', 'NN')],\n",
       " [(u'frozen', 'VBN'), (u'feet', 'NN')]]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map(lambda x: nltk.pos_tag(x[0]), matches[:-6:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tagged candidate sentences:\n",
    "topma = map(lambda x: x[0], matches[:-6:-1])\n",
    "cands = map(lambda x: unicode(' '.join(query)) + ' ' + x[1], topma)\n",
    "tcands = map(lambda x: nltk.pos_tag(nltk.tokenize.word_tokenize(x)), cands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(u'I', 'PRP'),\n",
       "  (u'would', 'MD'),\n",
       "  (u'like', 'VB'),\n",
       "  (u'to', 'TO'),\n",
       "  (u'buy', 'VB'),\n",
       "  (u'one', 'CD'),\n",
       "  (u'can', 'MD'),\n",
       "  (u'of', 'IN'),\n",
       "  (u'frozen', 'VBN'),\n",
       "  (u'maid', 'VBN')],\n",
       " [(u'I', 'PRP'),\n",
       "  (u'would', 'MD'),\n",
       "  (u'like', 'VB'),\n",
       "  (u'to', 'TO'),\n",
       "  (u'buy', 'VB'),\n",
       "  (u'one', 'CD'),\n",
       "  (u'can', 'MD'),\n",
       "  (u'of', 'IN'),\n",
       "  (u'frozen', 'VBN'),\n",
       "  (u'for', 'IN')],\n",
       " [(u'I', 'PRP'),\n",
       "  (u'would', 'MD'),\n",
       "  (u'like', 'VB'),\n",
       "  (u'to', 'TO'),\n",
       "  (u'buy', 'VB'),\n",
       "  (u'one', 'CD'),\n",
       "  (u'can', 'MD'),\n",
       "  (u'of', 'IN'),\n",
       "  (u'frozen', 'VBN'),\n",
       "  (u'into', 'IN')],\n",
       " [(u'I', 'PRP'),\n",
       "  (u'would', 'MD'),\n",
       "  (u'like', 'VB'),\n",
       "  (u'to', 'TO'),\n",
       "  (u'buy', 'VB'),\n",
       "  (u'one', 'CD'),\n",
       "  (u'can', 'MD'),\n",
       "  (u'of', 'IN'),\n",
       "  (u'frozen', 'VBN'),\n",
       "  (u'surface', 'NN')],\n",
       " [(u'I', 'PRP'),\n",
       "  (u'would', 'MD'),\n",
       "  (u'like', 'VB'),\n",
       "  (u'to', 'TO'),\n",
       "  (u'buy', 'VB'),\n",
       "  (u'one', 'CD'),\n",
       "  (u'can', 'MD'),\n",
       "  (u'of', 'IN'),\n",
       "  (u'frozen', 'VBN'),\n",
       "  (u'feet', 'NNS')]]"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?nltk.grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?nltk.parse.MaltParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grammar parser doesn't seem to be what people use, there is no way to specify deterministically a grammar for the entire english language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "possible way to filter the ngram matches: most likely pos tagset continuation, but how exactly.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?nltk.BigramTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training = nltk.corpus.treebank.tagged_sents()[:3522]\n",
    "tagger = nltk.BigramTagger(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  another way to use the nltk api:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  http://www.katrinerk.com/courses/python-worksheets/language-models-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am </s> <s> I am Sam I do not like green eggs and ham </s> <s> I am </s> <s> I am Sam I am Sam I do not like green eggs and ham </s> <s> Sam </s> <s> I do not like green eggs and ham </s> <s>\n",
      "which had no setting of its journey's end . This time tiptoeing in principle '' ? `` A good for an upright position . For one of our landing was almost worse than the disappearance of every jump we've made him , mai'teipa . But it , said . If\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# an nltk.FreqDist() is like a dictionary,\n",
    "# but it is ordered by frequency.\n",
    "# Also, nltk automatically fills the dictionary\n",
    "# with counts when given a list of words.\n",
    "\n",
    "freq_brown = nltk.FreqDist(brown.words())\n",
    "\n",
    "freq_brown.keys()\n",
    "freq_brown.items()[:20]\n",
    "\n",
    "# an nltk.ConditionalFreqDist() counts frequencies of pairs.\n",
    "# When given a list of bigrams, it maps each first word of a bigram\n",
    "# to a FreqDist over the second words of the bigram.\n",
    "\n",
    "cfreq_brown_2gram = nltk.ConditionalFreqDist(nltk.bigrams(brown.words()))\n",
    "\n",
    "# conditions() in a ConditionalFreqDist are like keys()\n",
    "# in a dictionary\n",
    "\n",
    "cfreq_brown_2gram.conditions()\n",
    "\n",
    "# the cfreq_brown_2gram entry for \"my\" is a FreqDist.\n",
    "\n",
    "cfreq_brown_2gram[\"my\"]\n",
    "\n",
    "# here are the words that can follow after \"my\".\n",
    "# We first access the FreqDist associated with \"my\",\n",
    "# then the keys in that FreqDist\n",
    "\n",
    "cfreq_brown_2gram[\"my\"].keys()\n",
    "\n",
    "# here are the 20 most frequent words to come after \"my\", with their frequencies\n",
    "\n",
    "cfreq_brown_2gram[\"my\"].items()[:20]\n",
    "\n",
    "# an nltk.ConditionalProbDist() maps pairs to probabilities.\n",
    "# One way in which we can do this is by using Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "cprob_brown_2gram = nltk.ConditionalProbDist(cfreq_brown_2gram, nltk.MLEProbDist)\n",
    "\n",
    "# This again has conditions() wihch are like dictionary keys\n",
    "\n",
    "cprob_brown_2gram.conditions()\n",
    "\n",
    "# Here is what we find for \"my\": a Maximum Likelihood Estimation-based probability distribution,\n",
    "# as a MLEProbDist object.\n",
    "\n",
    "cprob_brown_2gram[\"my\"]\n",
    "\n",
    "# We can find the words that can come after \"my\" by using the function samples()\n",
    "\n",
    "cprob_brown_2gram[\"my\"].samples()\n",
    "\n",
    "# Here is the probability of a particular pair:\n",
    "\n",
    "cprob_brown_2gram[\"my\"].prob(\"own\")\n",
    "\n",
    "# and we can draw a random word to follow \"my\"\n",
    "# based on the probabilities of the bigrams\n",
    "\n",
    "cprob_brown_2gram[\"my\"].generate()\n",
    "\n",
    "# We can use this to generate text at random\n",
    "# based on a given text of bigrams.\n",
    "# Let's do this for the Sam \"corpus\"\n",
    "\n",
    "corpus = \"\"\"<s> I am Sam </s>\n",
    "<s> Sam I am </s>\n",
    "<s> I do not like green eggs and ham </s>\"\"\"\n",
    "\n",
    "words = corpus.split()\n",
    "cfreq_sam = nltk.ConditionalFreqDist(nltk.bigrams(words))\n",
    "cprob_sam = nltk.ConditionalProbDist(cfreq_sam, nltk.MLEProbDist)\n",
    "\n",
    "word = \"<s>\"\n",
    "for index in range(50):\n",
    "    word = cprob_sam[ word].generate()\n",
    "    print word,\n",
    "print\n",
    "\n",
    "# Not a lot of variety. We need a bigger corpus.\n",
    "# What kind of genres do we have in the Brown corpus?\n",
    "brown.categories()    \n",
    "\n",
    "# Let's try Science Fiction.\n",
    "cfreq_scifi = nltk.ConditionalFreqDist(nltk.bigrams(brown.words(categories = \"science_fiction\")))\n",
    "cprob_scifi = nltk.ConditionalProbDist(cfreq_scifi, nltk.MLEProbDist)\n",
    "\n",
    "word = \"in\"\n",
    "for index in range(50):\n",
    "    word = cprob_scifi[ word ].generate()\n",
    "    print word,\n",
    "print\n",
    "\n",
    "# try this with other Brown corpus categories.\n",
    "\n",
    "# For the nltk.book objects, there is a generate() function.\n",
    "from nltk.book import *\n",
    "# text6.generate()\n",
    "# text7.generate()\n",
    "# text2.generate()\n",
    "\n",
    "# Do you think they used bigrams like we did earlier, or some larger n-grams?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
